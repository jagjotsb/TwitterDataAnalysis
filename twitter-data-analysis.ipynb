{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-24T15:29:14.692735Z","iopub.execute_input":"2022-05-24T15:29:14.693136Z","iopub.status.idle":"2022-05-24T15:29:14.821769Z","shell.execute_reply.started":"2022-05-24T15:29:14.693023Z","shell.execute_reply":"2022-05-24T15:29:14.820776Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **PART 1**\n\n1. Counting the total number of tweets, describing how I dealt with duplicates or other anomalies in the data set.\n2. Plotting a time-series of the number of tweets by day using the whole dataset and comment on what you see.\n3. Using a box and whisker diagram, compare the average number of tweets on the weekdays in the dataset to the numbers for weekend days. Are there statistically significant differences between the number of tweets on weekdays and weekends? \n4. Plot the average number of tweets at each hour of the day for weekdays and weekends and comment. You should have two plots where the x-axis is time of day (from midnight to midnight) and the y-axis shows the number of tweets.","metadata":{}},{"cell_type":"markdown","source":"# Data Extraction and reduction:\nThe twitter data files have a lot of data which will not be needed for tasks to complete. So to make it viable to use with the available RAM(16 GB) the data is extracted into smaller jsons. Once the data is collected it can be extracted again to make a dataframe out of it to perform further analysis. \n\nEach ID has been taken as a unique tweet, this will consider:\n* Same tweets done by the same user multiple times, as it still shows the activity.\n* Same tweets done by the different user, it could be bots or avid followers, which again shows the real world activity.\n* Retweets. That is, the same tweet echoed by differnt users. \n\nAlso below considerations have been taken in account:\n\n* The tweets which are considered ***duplicates*** are the data lines with same IDs, which will be removed in the following steps.\n* And the tweets with no ID at all have not been included in the count and have been considered as ***anomalies*** in the data. ","metadata":{}},{"cell_type":"markdown","source":"## Task 1.2: Total number of tweets \nThe smaller jsons have been downloaded and now will be used to make a dataframe which can be used to perform analysis. The first step of the analysis would be to calculate the number of tweets, before it can be done the ***duplicates*** with same ID will be removed. ","metadata":{}},{"cell_type":"code","source":"paths = []\nfor directory, _, files in os.walk('../input/twitter-task'):\n    for file in files:\n        paths.append(os.path.join(directory, file))\npaths = sorted(paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data for Task 1\n1. ID\n2. Created at\n3. language","metadata":{}},{"cell_type":"code","source":"import json\ni = 1\nfor path in paths:\n    \n    with open(path) as try_file:\n        #json_obj = json.load(try_file)\n        data = {'id':[], 'created_at':[],# 'text':[], \n                'tweet_lang':[], }#'tweet_coordinates':[], 'country':[], 'username':[], 'user_mentions':[]}\n        for line in try_file:\n            jsonfile= json.loads(line)\n\n            if 'id_str' in line:\n                data['id'].append(jsonfile['id_str'])\n            else:\n                continue\n            data['created_at'].append(jsonfile['created_at']) \n            #data['text'].append(jsonfile['text'].replace('\\n',''))\n            data['tweet_lang'].append(jsonfile['lang'])\n            \n        \n    name = './jsonq3'+str(i)+'.json' \n    df1 = pd.DataFrame(data)\n    jfile = df1.to_json(name, orient= 'records', date_format = 'iso')\n    i += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\nrjpaths = []\nfor directory, _, files in os.walk('../input/q1-json'):\n    for file in files:\n        if file == 'random.csv':\n            continue\n        else:\n            rjpaths.append(os.path.join(directory, file))\nrjpaths = sorted(rjpaths)\n\nfor rjpath in rjpaths:\n    df1 = pd.read_json(rjpath, orient = 'records', convert_dates = False)\n    #read_csv(rjpath, index_col = 0, parse_dates = ['created_at'], infer_datetime_format =True, dtype = {'id': 'str','created_at': 'str', 'text': 'str', 'tweet_lang': 'str','tweet_coordinates': 'str','country': 'str', 'username': 'str','user_mentions': 'str' } ) \n    df = df.append(df1)\n    del df1","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:12:49.806654Z","iopub.execute_input":"2022-05-24T16:12:49.806968Z","iopub.status.idle":"2022-05-24T16:16:01.916754Z","shell.execute_reply.started":"2022-05-24T16:12:49.806936Z","shell.execute_reply":"2022-05-24T16:16:01.915551Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('../input/csvwithtext/csvwtxt10.csv', index_col = 0, parse_dates = ['created_at'], \n                      infer_datetime_format =True, dtype = {'id': 'str','created_at': 'str', 'text': 'str', \n                                                            'tweet_lang': 'str',\n                               'tweet_coordinates': 'str','country': 'str', 'username': 'str','user_mentions': 'str' } )\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:10:18.167228Z","iopub.execute_input":"2022-05-24T16:10:18.167503Z","iopub.status.idle":"2022-05-24T16:10:36.879523Z","shell.execute_reply.started":"2022-05-24T16:10:18.167476Z","shell.execute_reply":"2022-05-24T16:10:36.878425Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df1 = df1.drop_duplicates(subset= ['id'])\ndf1.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:11:13.423936Z","iopub.execute_input":"2022-05-24T16:11:13.424390Z","iopub.status.idle":"2022-05-24T16:11:13.909314Z","shell.execute_reply.started":"2022-05-24T16:11:13.424356Z","shell.execute_reply":"2022-05-24T16:11:13.908487Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['id'] = df['id'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:22:58.379669Z","iopub.execute_input":"2022-05-24T16:22:58.380036Z","iopub.status.idle":"2022-05-24T16:23:28.201246Z","shell.execute_reply.started":"2022-05-24T16:22:58.380001Z","shell.execute_reply":"2022-05-24T16:23:28.200151Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Count before removing duplicates:\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:23:28.202850Z","iopub.execute_input":"2022-05-24T16:23:28.203130Z","iopub.status.idle":"2022-05-24T16:23:28.210118Z","shell.execute_reply.started":"2022-05-24T16:23:28.203096Z","shell.execute_reply":"2022-05-24T16:23:28.209095Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Count after removing duplicates:\ndf = df.drop_duplicates(subset= ['id'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-24T16:23:28.211196Z","iopub.execute_input":"2022-05-24T16:23:28.211477Z","iopub.status.idle":"2022-05-24T16:24:15.705356Z","shell.execute_reply.started":"2022-05-24T16:23:28.211441Z","shell.execute_reply":"2022-05-24T16:24:15.704383Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"***Total number of tweets:*** After removing 22219 duplicate IDs the final number of tweets came out to be ***24780899***. ","metadata":{}},{"cell_type":"markdown","source":"# Task 1.3: Tweets by day\nThe dataframe we have contains columns we do not need and so selective columns will be taken to analyse the per day tweet count. Once that is done we can count tweets per day grouping dataframe columns against \"created_at\" column.","metadata":{}},{"cell_type":"code","source":"df['date'] = df.created_at.astype(str).str[:10]\ndd = df['date'].value_counts()\n\ndfplot = dd.to_frame(name = 'count')\ndfplot.index.name = 'date'\ndfplot = dfplot.reset_index()\ndfplot['day'] = dfplot['date'].astype(str).str[:3]\ndfplot","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:24:50.135332Z","iopub.execute_input":"2022-05-24T18:24:50.135699Z","iopub.status.idle":"2022-05-24T18:24:50.153189Z","shell.execute_reply.started":"2022-05-24T18:24:50.135621Z","shell.execute_reply":"2022-05-24T18:24:50.152577Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"dayofweek = []\nfor index, rows in dfplot.iterrows():\n    if rows['day'] == 'Sun' or rows['day'] == 'Sat':\n        dayofweek.append('Weekend')\n    else:\n        dayofweek.append('Weekday')\ndfplot['dow'] = dayofweek\ndfplot","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:24:58.004403Z","iopub.execute_input":"2022-05-24T18:24:58.005330Z","iopub.status.idle":"2022-05-24T18:24:58.028252Z","shell.execute_reply.started":"2022-05-24T18:24:58.005271Z","shell.execute_reply":"2022-05-24T18:24:58.027037Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"dfplot['date'] = dfplot['date'].astype(str).str[4:]\ndfplot = dfplot.sort_values('date')\ndfplot = dfplot.reset_index(drop = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:26:19.013327Z","iopub.execute_input":"2022-05-24T18:26:19.013683Z","iopub.status.idle":"2022-05-24T18:26:19.019718Z","shell.execute_reply.started":"2022-05-24T18:26:19.013651Z","shell.execute_reply":"2022-05-24T18:26:19.018665Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"dfplot.T","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:26:29.455884Z","iopub.execute_input":"2022-05-24T18:26:29.457054Z","iopub.status.idle":"2022-05-24T18:26:29.485340Z","shell.execute_reply.started":"2022-05-24T18:26:29.456999Z","shell.execute_reply":"2022-05-24T18:26:29.484661Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"plot = dfplot.reset_index().plot.line(x = 'date', y = 'count', figsize = (20, 7), marker = 'o',\n                                      title = 'Daily tweet count in Europe for March 2020', x_compat=True)\nylabel = plot.set_ylabel('tweet count')\nmeanine = plot.axhline(y=dfplot['count'].mean(), color='r', linestyle='-', label = 'average tweets')\n#plt.savefig('#Daily_tweet_count_Europe_March2020.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:26:32.812836Z","iopub.execute_input":"2022-05-24T18:26:32.813127Z","iopub.status.idle":"2022-05-24T18:26:33.031878Z","shell.execute_reply.started":"2022-05-24T18:26:32.813098Z","shell.execute_reply":"2022-05-24T18:26:33.030665Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"***Comments*:** The number of tweets can be seen increasing after first 10 days of the month and can be seen that last 18 days have had above the average number of tweets. The tipping point of the increase seems to be 11th March. ","metadata":{}},{"cell_type":"markdown","source":"# Task 1.4: Tweets by language.\nFinding number of tweets by language will require the same approach as finding the tweets by date. This time the group by action will be performed on 'tweet_lang' column.\n\nLanguage info: According to twitter documentation twitter recognises 70 languages. These languages will be mapped against their encoder and will be translated to human recognisable words. Also the encoder 'und' is used for tweets which are not understandable or just contain symbols, emojis or tags. \n\n***BCP47*** library will be imported to translate language names from the encoded tags.","metadata":{}},{"cell_type":"code","source":"dflang = df.value_counts('tweet_lang')\ndflang = dflang.reset_index()\ndflang = dflang.rename(columns={0:'count'})","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:28:31.295093Z","iopub.execute_input":"2022-05-24T18:28:31.296329Z","iopub.status.idle":"2022-05-24T18:28:37.785328Z","shell.execute_reply.started":"2022-05-24T18:28:31.296287Z","shell.execute_reply":"2022-05-24T18:28:37.784330Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"! pip install bcp47","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:33:43.825607Z","iopub.execute_input":"2022-05-24T18:33:43.825937Z","iopub.status.idle":"2022-05-24T18:33:58.720284Z","shell.execute_reply.started":"2022-05-24T18:33:43.825907Z","shell.execute_reply":"2022-05-24T18:33:58.719140Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"import bcp47\n#dflang = dflang.reset_index()\nlanglist = []\nfor index, rows in dflang.iterrows():\n    if bcp47.tags.get(rows['tweet_lang']) != None:\n        langlist.append(bcp47.tags.get(rows['tweet_lang']))\n    else:\n        langlist.append(rows['tweet_lang'])\n        \n#since some of the language tags are not updated in the library, \n#they are manually added refering the details on twitter api guide\n\nlanglist[langlist.index('ckb')] = 'Sorani Kurdish'\nlanglist[langlist.index('ht')]  = 'Haitian Creole'\nlanglist[langlist.index('in')]  = 'Indonesian'\nlanglist[langlist.index('iw')]  = 'Hebrew'\nlanglist[langlist.index('tl')]  = 'Tagalog'\nlanglist[langlist.index('und')] = 'Undetermined'\n\ndflang['decoded_lang'] = langlist","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:34:32.482420Z","iopub.execute_input":"2022-05-24T18:34:32.482786Z","iopub.status.idle":"2022-05-24T18:34:32.497360Z","shell.execute_reply.started":"2022-05-24T18:34:32.482752Z","shell.execute_reply":"2022-05-24T18:34:32.496661Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"dflang","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:34:35.874903Z","iopub.execute_input":"2022-05-24T18:34:35.875196Z","iopub.status.idle":"2022-05-24T18:34:35.892660Z","shell.execute_reply.started":"2022-05-24T18:34:35.875158Z","shell.execute_reply":"2022-05-24T18:34:35.891935Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"\nlangplot = dflang.plot.bar(x = 'decoded_lang', y= 'count', figsize = [17,8], title = 'Popular languages on twitter in Europe')\n#plt.savefig('#Languages_used_bargraph.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:35:17.854190Z","iopub.execute_input":"2022-05-24T18:35:17.854511Z","iopub.status.idle":"2022-05-24T18:35:19.485662Z","shell.execute_reply.started":"2022-05-24T18:35:17.854480Z","shell.execute_reply":"2022-05-24T18:35:19.484637Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"***Comments***: A few languages seem much more used than the others. Looking at the bar chart one can determine that the geography in question is Europe as all the languages clearly towering are European languages. English, not surprisingly, is the most common language in the tweets. It can also be seen that tweets with language 'und' or 'undetermined' is also pretty common, this shows the trend that a huge number of people tend to tweet just the emojis or tags in their tweets.   ","metadata":{}},{"cell_type":"markdown","source":"# Task 1.5: Number of tweets during weekdays vs weekends \n\n","metadata":{}},{"cell_type":"code","source":"dfplot.boxplot(column=['count'], by= 'dow')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:36:43.563337Z","iopub.execute_input":"2022-05-24T18:36:43.563748Z","iopub.status.idle":"2022-05-24T18:36:43.786577Z","shell.execute_reply.started":"2022-05-24T18:36:43.563709Z","shell.execute_reply":"2022-05-24T18:36:43.785844Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=[20,10])\nax = sns.boxplot(x=\"dow\", y=\"count\", data=dfplot, width = 0.3)\nsns.set(font_scale=2)\n#plt.xlabel(fontdict = {'size' : 16})\nplt.title('Tweet count distribution on weekdays and weekends')#, fontdict = {'size' : 20})\nplt.savefig('#weekdayweekendDistribution.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:37:20.301798Z","iopub.execute_input":"2022-05-24T18:37:20.302094Z","iopub.status.idle":"2022-05-24T18:37:20.685076Z","shell.execute_reply.started":"2022-05-24T18:37:20.302063Z","shell.execute_reply":"2022-05-24T18:37:20.684475Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"***Comments:*** People seem to tweet more during the weekends.","metadata":{}},{"cell_type":"markdown","source":"# Task 1.6: Finding tweets by hour to determine busiest and least busy.","metadata":{}},{"cell_type":"code","source":"df['hour'] = df.created_at.astype(str).str[11:13]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:39:22.778951Z","iopub.execute_input":"2022-05-24T18:39:22.779730Z","iopub.status.idle":"2022-05-24T18:40:24.885456Z","shell.execute_reply.started":"2022-05-24T18:39:22.779687Z","shell.execute_reply":"2022-05-24T18:40:24.884256Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"df_hour = df.value_counts('hour')\ndf_hour = df_hour.to_frame(name= 'tweet_count')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:44:01.679630Z","iopub.execute_input":"2022-05-24T18:44:01.680985Z","iopub.status.idle":"2022-05-24T18:44:04.955209Z","shell.execute_reply.started":"2022-05-24T18:44:01.680925Z","shell.execute_reply":"2022-05-24T18:44:04.954233Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"df_hour = df_hour.reset_index()\ndf_hour = df_hour.sort_values('hour')\ndf_hour = df_hour.reset_index(drop= True)\ndf_hour","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:44:12.850957Z","iopub.execute_input":"2022-05-24T18:44:12.851303Z","iopub.status.idle":"2022-05-24T18:44:12.865344Z","shell.execute_reply.started":"2022-05-24T18:44:12.851264Z","shell.execute_reply":"2022-05-24T18:44:12.864236Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"df_hour['average_tweet_per_hour'] = df_hour['tweet_count']/31\ndf_hour","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:44:24.466277Z","iopub.execute_input":"2022-05-24T18:44:24.466638Z","iopub.status.idle":"2022-05-24T18:44:24.485842Z","shell.execute_reply.started":"2022-05-24T18:44:24.466580Z","shell.execute_reply":"2022-05-24T18:44:24.484951Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"df_hour.plot.bar(x = 'hour', y= 'average_tweet_per_hour', )","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:44:35.272236Z","iopub.execute_input":"2022-05-24T18:44:35.273341Z","iopub.status.idle":"2022-05-24T18:44:35.700120Z","shell.execute_reply.started":"2022-05-24T18:44:35.273287Z","shell.execute_reply":"2022-05-24T18:44:35.699100Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=[20,10])\nax = sns.barplot(x=\"hour\", y=\"average_tweet_per_hour\", data=df_hour)\nplt.title('Average Tweet count distribution per hour for March 2020')\nplt.savefig('#hourlyDistribution.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:44:51.515075Z","iopub.execute_input":"2022-05-24T18:44:51.515455Z","iopub.status.idle":"2022-05-24T18:44:52.328703Z","shell.execute_reply.started":"2022-05-24T18:44:51.515419Z","shell.execute_reply":"2022-05-24T18:44:52.327790Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"***Comments***: It can be seen that number of tweets per hour peak during the evening between 5 pm and 10 pm. This also aligns with the our daily routine of working men and women, most tweets are posted after office hours. We can also observe the steep depression in the count during wee hours which again tells the obvious, indicates that majority of users are sleeping.","metadata":{}},{"cell_type":"code","source":"#saving the ram!\ndel df\ndel df_hour\ndel dfplot\ndel dflang","metadata":{"execution":{"iopub.status.busy":"2022-05-24T18:47:03.402638Z","iopub.execute_input":"2022-05-24T18:47:03.403164Z","iopub.status.idle":"2022-05-24T18:47:03.425134Z","shell.execute_reply.started":"2022-05-24T18:47:03.403129Z","shell.execute_reply":"2022-05-24T18:47:03.423851Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"# Part 2. Mapping \n1. Drawing a map of Europe showing the location of the GPS-tagged tweets - these are tweets which have a “coordinates” field in the metadata. \n2. Explaining any patterns I observed.","metadata":{}},{"cell_type":"markdown","source":"\n**Data for task 2**\n\n* ID\n* Coordinates\n","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport os\n\ni = 1\nfor path in paths:\n    \n    with open(path) as try_file:\n        data = {'id':[],'tweet_coordinates':[], 'country': [], 'country_code': [] }\n                 \n        for line in try_file:\n            jsonfile= json.loads(line)\n            \n            #extracting the data if it has ID and coordinates\n            if ('id_str' in line) and (jsonfile['coordinates'] != None ):\n                data['id'].append(jsonfile['id_str'])\n                data['tweet_coordinates'].append(jsonfile['coordinates']['coordinates'])\n                if jsonfile['place'] != None:\n                    data['country'].append(jsonfile['place']['country'])\n                    data['country_code'].append(jsonfile['place']['country_code'])\n                else:\n                    data['country'].append(np.nan)\n                    data['country_code'].append(np.nan)\n            else:\n                continue\n\n        \n    name = './jsonq2'+str(i)+'.json' \n    df1 = pd.DataFrame(data)\n    jfile = df1.to_json(name, orient= 'records', date_format = 'iso')\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport os\ndf = pd.DataFrame()\nrjpaths = []\nfor directory, _, files in os.walk('../input/q2-json'):\n    for file in files:\n        rjpaths.append(os.path.join(directory, file))\nrjpaths = sorted(rjpaths)\n\nfor rjpath in rjpaths:\n    df1 = pd.read_json(rjpath, orient = 'records', convert_dates = False)\n    df = df.append(df1)\n    del df1","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:38:38.002408Z","iopub.execute_input":"2022-05-24T20:38:38.002767Z","iopub.status.idle":"2022-05-24T20:38:54.803898Z","shell.execute_reply.started":"2022-05-24T20:38:38.002673Z","shell.execute_reply":"2022-05-24T20:38:54.803036Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:39:33.564227Z","iopub.execute_input":"2022-05-24T20:39:33.564507Z","iopub.status.idle":"2022-05-24T20:39:33.584541Z","shell.execute_reply.started":"2022-05-24T20:39:33.564479Z","shell.execute_reply":"2022-05-24T20:39:33.583873Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates(subset = ['id'])\ndf.index = df.id\ndf = df.drop(columns = ['id'])\ndf = df.dropna()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:39:53.740735Z","iopub.execute_input":"2022-05-24T20:39:53.741046Z","iopub.status.idle":"2022-05-24T20:39:54.266148Z","shell.execute_reply.started":"2022-05-24T20:39:53.741010Z","shell.execute_reply":"2022-05-24T20:39:54.265239Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"longarr =[df.iloc[i,0][0] for i in range(len(df.index))]\nlatarr = [df.iloc[i,0][1] for i in range(len(df.index))]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:40:39.272028Z","iopub.execute_input":"2022-05-24T20:40:39.272345Z","iopub.status.idle":"2022-05-24T20:42:12.796988Z","shell.execute_reply.started":"2022-05-24T20:40:39.272311Z","shell.execute_reply":"2022-05-24T20:42:12.796113Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['long'] = longarr\ndf['lat'] = latarr\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:42:12.798774Z","iopub.execute_input":"2022-05-24T20:42:12.799119Z","iopub.status.idle":"2022-05-24T20:42:13.350001Z","shell.execute_reply.started":"2022-05-24T20:42:12.799061Z","shell.execute_reply":"2022-05-24T20:42:13.349143Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"***Comment:*** Because names of some countries are written in local languages, using country codes to get the names of the countries.","metadata":{}},{"cell_type":"code","source":"df.iloc[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:42:24.383793Z","iopub.execute_input":"2022-05-24T20:42:24.384099Z","iopub.status.idle":"2022-05-24T20:42:24.390008Z","shell.execute_reply.started":"2022-05-24T20:42:24.384050Z","shell.execute_reply":"2022-05-24T20:42:24.389034Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pycountry\ncountries = []\n\n#get the values of countries against ISO tags if they are present in pycountry library\nfor index, rows in df.iterrows():\n    if rows['country_code'] != None:\n        if pycountry.countries.get(alpha_2= rows['country_code']) != None:\n            countries.append(pycountry.countries.get(alpha_2= rows['country_code']).name)\n        else: \n            countries.append(rows['country_code'])\n    else:\n        countries.append(np.nan)\n\n#Kosovo was not present in the pyountry and hence manually added\ndf['decoded_C'] = ['Kosovo' if ca == 'XK' else ca for ca in countries]\n\ndf = df.dropna()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:42:49.067874Z","iopub.execute_input":"2022-05-24T20:42:49.068487Z","iopub.status.idle":"2022-05-24T20:44:22.057216Z","shell.execute_reply.started":"2022-05-24T20:42:49.068443Z","shell.execute_reply":"2022-05-24T20:44:22.056158Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"locationlist = df[['lat','long']].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:47:26.552096Z","iopub.execute_input":"2022-05-24T20:47:26.552744Z","iopub.status.idle":"2022-05-24T20:47:29.116729Z","shell.execute_reply.started":"2022-05-24T20:47:26.552703Z","shell.execute_reply":"2022-05-24T20:47:29.115850Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Task 2.1: Plotting a map.\n\n","metadata":{}},{"cell_type":"code","source":"#we'll need 3 dataframes for this, \n#1. to pin point the tweets, \n#2. to get the number of tweets for each country for choropleth\n#3. The geo json to get the boundries for each country\n\n#2nd data frame for choropleth\n\ndf_tweets = df.groupby('decoded_C').count()\ndf_tweets = df_tweets.drop(['tweet_coordinates', 'country', 'long', 'lat'], axis = 1)\ndf_tweets = df_tweets.reset_index()\ndf_tweets = df_tweets.drop(0)\ndf_tweets","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:49:39.211104Z","iopub.execute_input":"2022-05-24T20:49:39.211727Z","iopub.status.idle":"2022-05-24T20:49:40.237554Z","shell.execute_reply.started":"2022-05-24T20:49:39.211670Z","shell.execute_reply":"2022-05-24T20:49:40.236535Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_tweets.sort_values(by=['country_code']).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:49:48.383330Z","iopub.execute_input":"2022-05-24T20:49:48.384218Z","iopub.status.idle":"2022-05-24T20:49:48.393919Z","shell.execute_reply.started":"2022-05-24T20:49:48.384176Z","shell.execute_reply":"2022-05-24T20:49:48.393260Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# adding geo json file\n\nworldGeo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import folium\nworldGeo = f\"../input/world-countries-geo-json/world-countries.geo.json\"\nm = folium.Map(location=[51.1657,10.4515], zoom_start=3,  tiles=\"cartodbpositron\")\nfolium.Choropleth(\n    geo_data=worldGeo,\n    name=\"choropleth\",\n    data=df_tweets,\n    columns=[\"decoded_C\", \"country_code\"],\n    key_on=\"feature.properties.name\",\n    fill_color=\"RdPu\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name=\"Tweet count in each country\",\n).add_to(m)\n\nfolium.LayerControl().add_to(m)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:50:22.823832Z","iopub.execute_input":"2022-05-24T20:50:22.824275Z","iopub.status.idle":"2022-05-24T20:50:23.137043Z","shell.execute_reply.started":"2022-05-24T20:50:22.824242Z","shell.execute_reply":"2022-05-24T20:50:23.136154Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"m","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:50:29.521658Z","iopub.execute_input":"2022-05-24T20:50:29.522184Z","iopub.status.idle":"2022-05-24T20:50:29.684217Z","shell.execute_reply.started":"2022-05-24T20:50:29.522148Z","shell.execute_reply":"2022-05-24T20:50:29.683211Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from folium.plugins import MarkerCluster\n\nmarker_cluster = MarkerCluster().add_to(m)\n\nfor point in range(0, len(locationlist)):\n    folium.Marker(locationlist[point]).add_to(marker_cluster)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:50:48.819698Z","iopub.execute_input":"2022-05-24T20:50:48.819978Z","iopub.status.idle":"2022-05-24T20:51:23.951352Z","shell.execute_reply.started":"2022-05-24T20:50:48.819947Z","shell.execute_reply":"2022-05-24T20:51:23.950338Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"folium.Marker([41.54902754, 2.10348329]).add_to(marker_cluster)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:51:23.953425Z","iopub.execute_input":"2022-05-24T20:51:23.953815Z","iopub.status.idle":"2022-05-24T20:51:23.963173Z","shell.execute_reply.started":"2022-05-24T20:51:23.953769Z","shell.execute_reply":"2022-05-24T20:51:23.960473Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"m.save(\"./index.html\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom matplotlib.patches import Circle\nplt.style.use('fivethirtyeight')\nplt.rcParams.update({'font.size': 20})\nplt.rcParams['figure.figsize'] = (20, 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:00:18.240719Z","iopub.execute_input":"2022-05-24T21:00:18.241038Z","iopub.status.idle":"2022-05-24T21:00:18.700296Z","shell.execute_reply.started":"2022-05-24T21:00:18.241004Z","shell.execute_reply":"2022-05-24T21:00:18.699109Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"ax = plt.axes(projection=ccrs.PlateCarree())\nax.stock_img()\n# plot individual locations                                                                                                       \nax.plot(df.long, df.lat, 'ro', transform=ccrs.PlateCarree())\n# add coastlines for reference                                                                                                \nax.coastlines(resolution='50m')\nax.set_global()\nax.set_extent([20, -20, 45,60])\n'''def get_radius(freq):\n    if freq < 50:\n        return 0.5\n    elif freq < 200:\n        return 1.2\n    elif freq < 1000:\n        return 1.8\n# plot count of tweets per location\nfor i,x in locations.iteritems():\n    ax.add_patch(Circle(xy=[i[2], i[1]], radius=get_radius(x), color='blue', alpha=0.6, transform=ccrs.PlateCarree()))'''\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:00:20.586824Z","iopub.execute_input":"2022-05-24T21:00:20.587157Z","iopub.status.idle":"2022-05-24T21:00:25.751293Z","shell.execute_reply.started":"2022-05-24T21:00:20.587118Z","shell.execute_reply":"2022-05-24T21:00:25.750444Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Part 3. Users \n1. Making a histogram of tweets per user with number of users on the y-axis and number of tweets they make on the x-axis. Discuss the distribution that you see. All the users in the data set should be included! \n2. Find the top-5 users by total number of tweets. Do you think any are automated accounts (aka. bots)? Justify your answer.\n3. Find the 5 users who receive the most mentions and comment on this.\n4. Calculate how often users in the UK, France, Germany, Italy and Turkeymention users in each of the other 4 countries. You should compute 25 numbers e.g. UK mentions UK, UK mentions France, France mentions UK etc. Comment on any patterns you observe. ","metadata":{}},{"cell_type":"markdown","source":"# Data for Task 3\n1. ID\n2. Created at\n3. Users\n4. User mentions","metadata":{}},{"cell_type":"code","source":"import json\ni = 1\nfor path in paths:\n    \n    with open(path) as try_file:\n        #json_obj = json.load(try_file)\n        data = {'id':[], 'created_at':[],# 'text':[], 'tweet_lang':[], }#'tweet_coordinates':[], 'country':[], \n                'username':[], 'user_mentions':[]}\n        for line in try_file:\n            jsonfile= json.loads(line)\n\n            if 'id_str' in line:\n                data['id'].append(jsonfile['id_str'])\n            else:\n                continue\n            data['created_at'].append(jsonfile['created_at']) \n            data['username'].append(jsonfile['user']['screen_name'])\n\n            if len(jsonfile['entities']['user_mentions']) == 0:\n                data['user_mentions'].append(np.nan)\n            else:\n                data['user_mentions'].append([jsonfile['entities']['user_mentions'][i]['screen_name'] \n                                             for i in range(len(jsonfile['entities']['user_mentions']))])\n        \n    name = './jsonq3'+str(i)+'.json' \n    df1 = pd.DataFrame(data)\n    jfile = df1.to_json(name, orient= 'records', date_format = 'iso')\n    i += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\nrjpaths = []\nfor directory, _, files in os.walk('../input/q3-json'):\n    for file in files:\n        rjpaths.append(os.path.join(directory, file))\nrjpaths = sorted(rjpaths)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:01:11.543349Z","iopub.execute_input":"2022-05-24T21:01:11.544117Z","iopub.status.idle":"2022-05-24T21:01:11.560130Z","shell.execute_reply.started":"2022-05-24T21:01:11.544077Z","shell.execute_reply":"2022-05-24T21:01:11.559249Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for rjpath in rjpaths:\n    df1 = pd.read_json(rjpath, orient = 'records', convert_dates = False)\n    #read_csv(rjpath, index_col = 0, parse_dates = ['created_at'], infer_datetime_format =True, dtype = {'id': 'str','created_at': 'str', 'text': 'str', 'tweet_lang': 'str','tweet_coordinates': 'str','country': 'str', 'username': 'str','user_mentions': 'str' } ) \n    df = df.append(df1)\n    del df1","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:01:34.044190Z","iopub.execute_input":"2022-05-24T21:01:34.044945Z","iopub.status.idle":"2022-05-24T21:05:47.760060Z","shell.execute_reply.started":"2022-05-24T21:01:34.044902Z","shell.execute_reply":"2022-05-24T21:05:47.759162Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Task 3.1** \nNumber of tweets per user bar graph","metadata":{}},{"cell_type":"code","source":"df = df.drop_duplicates(subset = ['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:06:16.477692Z","iopub.execute_input":"2022-05-24T21:06:16.478462Z","iopub.status.idle":"2022-05-24T21:06:30.714143Z","shell.execute_reply.started":"2022-05-24T21:06:16.478400Z","shell.execute_reply":"2022-05-24T21:06:30.713283Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_usertweets = df.groupby('username').count()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:08:03.509711Z","iopub.execute_input":"2022-05-24T21:08:03.510149Z","iopub.status.idle":"2022-05-24T21:08:30.521096Z","shell.execute_reply.started":"2022-05-24T21:08:03.510054Z","shell.execute_reply":"2022-05-24T21:08:30.520084Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_usertweets = df_usertweets.reset_index()\ndf_usertweets = df_usertweets.drop(['created_at', 'user_mentions'], axis =1)\ndf_usertweets = df_usertweets.rename(columns = {'id' : 'Tweet_count'})\ndf_usertweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:08:34.098870Z","iopub.execute_input":"2022-05-24T21:08:34.099222Z","iopub.status.idle":"2022-05-24T21:08:34.618189Z","shell.execute_reply.started":"2022-05-24T21:08:34.099186Z","shell.execute_reply":"2022-05-24T21:08:34.616768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df_usertweets[df_usertweets['Tweet_count'] < 5]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:08:42.601464Z","iopub.execute_input":"2022-05-24T21:08:42.601770Z","iopub.status.idle":"2022-05-24T21:08:42.700573Z","shell.execute_reply.started":"2022-05-24T21:08:42.601738Z","shell.execute_reply":"2022-05-24T21:08:42.699443Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"***Comments:*** Total number of users that tweeted in that month are **1111170** in number, looking at the plot it can be seen that there is a huge majority of the user post less than 5 tweets a month. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nf, ax = plt.subplots(figsize=(15, 7))\nsns.histplot(data = df_usertweets,x = 'Tweet_count', binwidth = 5)\nplt.ylabel('Number of users')\nplt.title('Number of tweets per user')\nplt.xlim(0,150)\n#plt.savefig('#3.1_tweets_per_user.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:09:10.094674Z","iopub.execute_input":"2022-05-24T21:09:10.095522Z","iopub.status.idle":"2022-05-24T21:09:23.679174Z","shell.execute_reply.started":"2022-05-24T21:09:10.095473Z","shell.execute_reply":"2022-05-24T21:09:23.678169Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 7))\nsns.histplot(data = df_usertweets,x = 'Tweet_count', binwidth = 10)\nplt.ylabel('Number of users')\nplt.title('Number of tweets per user')\nplt.xlim(0,2000)\nax.set_yscale('log')\nplt.savefig('#3.1_tweets_per_user.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:09:28.789492Z","iopub.execute_input":"2022-05-24T21:09:28.789949Z","iopub.status.idle":"2022-05-24T21:09:42.857798Z","shell.execute_reply.started":"2022-05-24T21:09:28.789896Z","shell.execute_reply":"2022-05-24T21:09:42.856746Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 7))\nsns.histplot(data = df_usertweets,x = 'Tweet_count', binwidth = 5)\nplt.ylabel('Number of users')\nplt.title('Number of tweets per user')\nplt.xlim(0,300)\nplt.ylim(0,10000)\nplt.savefig('#3.1_tweets_per_user.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:09:42.860217Z","iopub.execute_input":"2022-05-24T21:09:42.860554Z","iopub.status.idle":"2022-05-24T21:10:04.441453Z","shell.execute_reply.started":"2022-05-24T21:09:42.860506Z","shell.execute_reply":"2022-05-24T21:10:04.440152Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"***Comments***: Minority of user do majority of tweets!","metadata":{}},{"cell_type":"markdown","source":"**Task 3.2**\nHunting for the top 10 users","metadata":{}},{"cell_type":"code","source":"df_usertweets.sort_values(by = ['Tweet_count'], \n                          ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:11:16.487246Z","iopub.execute_input":"2022-05-24T21:11:16.487804Z","iopub.status.idle":"2022-05-24T21:11:16.880892Z","shell.execute_reply.started":"2022-05-24T21:11:16.487764Z","shell.execute_reply":"2022-05-24T21:11:16.880299Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"* WhatsOnOLIO = valid food sharing app, automated\n* KorayDavulcu2 and thaiselenags = possibly spamming or automated account, account doesn't exist \n* MathieuRonsard = possibly automated account, low followers and all the tweets redirect to a suspicious link.\n* infosrv: Account suspended by twitter,\n* AnimalsHolbox: 0 followers, 0 following, 2.3 million tweets, possibly spammer\n* HoraCatalana: Automated account posts time every 5 minutes in Catalan\n* _BB_RADIO_MUSIC: German radio posts all the songs it plays, possibly automated\n* RadioTeddyMusic: Another german radio station, posts all the songs it plays, possibly automated\n* haykakan_top: possibly automated\n","metadata":{}},{"cell_type":"markdown","source":"**Task 3.3** User mentions data.","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"namelist = df['user_mentions'].values.tolist()\n\nflat_list = []\nfor sublist in namelist:\n    for item in sublist:\n        flat_list.append(item)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:12:53.604771Z","iopub.execute_input":"2022-05-24T21:12:53.605392Z","iopub.status.idle":"2022-05-24T21:13:02.295977Z","shell.execute_reply.started":"2022-05-24T21:12:53.605338Z","shell.execute_reply":"2022-05-24T21:13:02.295061Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from collections import Counter, OrderedDict\ncount = Counter()\nfor user in flat_list:\n    count[user]+=1\ndf_mentionCount = pd.DataFrame.from_dict(count, orient = 'index', columns = ['MentionCount'])\ndf_mentionCount","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:13:31.208439Z","iopub.execute_input":"2022-05-24T21:13:31.208711Z","iopub.status.idle":"2022-05-24T21:13:55.436430Z","shell.execute_reply.started":"2022-05-24T21:13:31.208682Z","shell.execute_reply":"2022-05-24T21:13:55.435429Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nf,ax = plt.subplots(figsize = [15,7])\nsns.histplot(data = df_mentionCount, x = 'MentionCount', binwidth = 10)\nplt.xlim(0,100)\nplt.ylabel('Number of Users')\nplt.title('User mentions per user during March 2020')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:14:15.217670Z","iopub.execute_input":"2022-05-24T21:14:15.218287Z","iopub.status.idle":"2022-05-24T21:14:51.738010Z","shell.execute_reply.started":"2022-05-24T21:14:15.218233Z","shell.execute_reply":"2022-05-24T21:14:51.737020Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(figsize = [15,7])\nsns.histplot(data = df_mentionCount, x = 'MentionCount', binwidth = 10)\nplt.xlim(0,2500)\nax.set_yscale('log')\n#plt.ylim(0,700)\nplt.ylabel('Number of Users')\nplt.title('User mentions per user during March 2020')\nplt.savefig('#3.3_user_mentions per_user.png', bbox_inches = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:14:57.558670Z","iopub.execute_input":"2022-05-24T21:14:57.559004Z","iopub.status.idle":"2022-05-24T21:15:17.274832Z","shell.execute_reply.started":"2022-05-24T21:14:57.558970Z","shell.execute_reply":"2022-05-24T21:15:17.274158Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"***Comment:*** User that got more than 200 mentions are less than 700 out of **1111170** of the total users. ","metadata":{}},{"cell_type":"markdown","source":"**Task 3.4**\nSome of the highly mentioned users and why?\n","metadata":{}},{"cell_type":"code","source":"df_mentionCount.\\\nsort_values(by = ['MentionCount'], \n            ascending = False).head(15)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:15:25.634776Z","iopub.execute_input":"2022-05-24T21:15:25.635451Z","iopub.status.idle":"2022-05-24T21:15:26.388258Z","shell.execute_reply.started":"2022-05-24T21:15:25.635406Z","shell.execute_reply":"2022-05-24T21:15:26.387523Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"***Comments:*** Ah! Famous people!","metadata":{}},{"cell_type":"code","source":"#Alternatively\njsonResult = OrderedDict(count.most_common(10))\ntupleResult = count.most_common(10)\ntupleResult","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:16:09.210556Z","iopub.execute_input":"2022-05-24T21:16:09.211020Z","iopub.status.idle":"2022-05-24T21:16:11.140875Z","shell.execute_reply.started":"2022-05-24T21:16:09.210988Z","shell.execute_reply":"2022-05-24T21:16:11.140311Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4. Events \n1. Identify 3 days with unusually high activity in 3 different countries of your choosing. For example you could choose one day in the UK, one in France and one in Turkey. Describe and justify how you identify ‘unusual’ days. \n2. Characterise each of these three days. Exactly how you do this is up to you, but for example you could:\nDisplay some indicative Tweets.\nMake a word cloud from the tweet text.\nPlot tweets locations on a map.\nValidate your conclusions with some other source of data e.g. government or news reports.","metadata":{}},{"cell_type":"markdown","source":"# Data for Task 4\n* ID\n* Created at\n* Country: England, Ireland, \n* Text","metadata":{}},{"cell_type":"code","source":"import json\ni = 1\nfor path in paths:\n    \n    with open(path) as try_file:\n        data = {'id':[], 'created_at':[], 'text':[], 'country':[]}\n        for line in try_file:\n            jsonfile= json.loads(line)\n            if ('id_str' in line) and (jsonfile['place'] != None):\n                if(jsonfile['place']['country'] == 'United Kingdom' or jsonfile['place']['country'] == 'Ireland') :\n                    data['id'].append(jsonfile['id_str'])\n                    data['created_at'].append(jsonfile['created_at'])\n                    data['text'].append(jsonfile['text'].replace('\\n',' '))\n                    data['country'].append(jsonfile['place']['country'])\n                else:\n                    continue\n                    \n    name = './jsonq4'+str(i)+'.json' \n    df1 = pd.DataFrame(data)\n    jfile = df1.to_json(name, orient= 'records', date_format = 'iso')\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport os\n\ndf = pd.DataFrame()\nrjpaths = []\nfor directory, _, files in os.walk('../input/q4-json'):\n    for file in files:\n        rjpaths.append(os.path.join(directory, file))\nrjpaths = sorted(rjpaths)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:23:10.353256Z","iopub.execute_input":"2022-05-24T21:23:10.353722Z","iopub.status.idle":"2022-05-24T21:23:10.383162Z","shell.execute_reply.started":"2022-05-24T21:23:10.353666Z","shell.execute_reply":"2022-05-24T21:23:10.381510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for rjpath in rjpaths:\n    df1 = pd.read_json(rjpath, orient = 'records', convert_dates = False)\n    #read_csv(rjpath, index_col = 0, parse_dates = ['created_at'], infer_datetime_format =True, dtype = {'id': 'str','created_at': 'str', 'text': 'str', 'tweet_lang': 'str','tweet_coordinates': 'str','country': 'str', 'username': 'str','user_mentions': 'str' } ) \n    df = df.append(df1)\n    del df1","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:23:13.930193Z","iopub.execute_input":"2022-05-24T21:23:13.930530Z","iopub.status.idle":"2022-05-24T21:24:56.012911Z","shell.execute_reply.started":"2022-05-24T21:23:13.930497Z","shell.execute_reply":"2022-05-24T21:24:56.011426Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Task 4.1\nPlotting unusual days for UK and Ireland\n\nFirst plotting number of tweets per day for each country and checking spikes. From the previous analysis we observed that number of tweets during the weekends are more than on weekdays. Keeping this information in mind, we can check weekdays with high number of tweets. ","metadata":{}},{"cell_type":"code","source":"df['date'] = df['created_at'].str[4:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:25:00.595766Z","iopub.execute_input":"2022-05-24T21:25:00.596044Z","iopub.status.idle":"2022-05-24T21:25:08.106001Z","shell.execute_reply.started":"2022-05-24T21:25:00.596013Z","shell.execute_reply":"2022-05-24T21:25:08.105329Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing:\n\n* making all text lowercase\n* removing hyperlinks\n* removing user tags\n* removing stemming words: like, likes, liking, \n* removing non alpha numerics","metadata":{}},{"cell_type":"code","source":"df['processed_text'] = df['text'].str.lower().str.replace('(@[a-z0-9]+)\\w+',' ')\\\n                        .str.replace('(http\\S+)', ' ')\\\n                          .str.replace('([^0-9a-z \\t])',' ').str.replace(' +',' ')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:25:32.327380Z","iopub.execute_input":"2022-05-24T21:25:32.327777Z","iopub.status.idle":"2022-05-24T21:27:01.994911Z","shell.execute_reply.started":"2022-05-24T21:25:32.327731Z","shell.execute_reply":"2022-05-24T21:27:01.993608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:01.997438Z","iopub.execute_input":"2022-05-24T21:27:01.997791Z","iopub.status.idle":"2022-05-24T21:27:04.782558Z","shell.execute_reply.started":"2022-05-24T21:27:01.997745Z","shell.execute_reply":"2022-05-24T21:27:04.781183Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_UK = df[df.country == 'United Kingdom']\ndf_IR = df[df.country == 'Ireland']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:04.784238Z","iopub.execute_input":"2022-05-24T21:27:04.784481Z","iopub.status.idle":"2022-05-24T21:27:09.940778Z","shell.execute_reply.started":"2022-05-24T21:27:04.784457Z","shell.execute_reply":"2022-05-24T21:27:09.939871Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_UK_count= df_UK.groupby('date').count()\ndf_UK_count = df_UK_count.drop(['created_at', 'text', 'country'], axis = 1)\ndf_UK_count = df_UK_count.reset_index()\ndf_UK_count = df_UK_count.rename(columns = {'id': 'Tweet_count_UK'})\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:09.942453Z","iopub.execute_input":"2022-05-24T21:27:09.943215Z","iopub.status.idle":"2022-05-24T21:27:15.109328Z","shell.execute_reply.started":"2022-05-24T21:27:09.943154Z","shell.execute_reply":"2022-05-24T21:27:15.107295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_IR_count= df_IR.groupby('date').count()\ndf_IR_count = df_IR_count.drop(['created_at', 'text', 'country'], axis = 1)\ndf_IR_count = df_IR_count.reset_index()\ndf_IR_count = df_IR_count.rename(columns = {'id': 'Tweet_count_ireland'})\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:15.112449Z","iopub.execute_input":"2022-05-24T21:27:15.112817Z","iopub.status.idle":"2022-05-24T21:27:16.401349Z","shell.execute_reply.started":"2022-05-24T21:27:15.112764Z","shell.execute_reply":"2022-05-24T21:27:16.399736Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_IR_count = df_IR_count.rename(columns = {'Tweet_count': 'Tweet_count_ireland'})\ndf_UK_count = df_UK_count.rename(columns = {'Tweet_count': 'Tweet_count_UK'})","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:16.403736Z","iopub.execute_input":"2022-05-24T21:27:16.404068Z","iopub.status.idle":"2022-05-24T21:27:16.413838Z","shell.execute_reply.started":"2022-05-24T21:27:16.404027Z","shell.execute_reply":"2022-05-24T21:27:16.412331Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_IR_count.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:16.415755Z","iopub.execute_input":"2022-05-24T21:27:16.416123Z","iopub.status.idle":"2022-05-24T21:27:16.446172Z","shell.execute_reply.started":"2022-05-24T21:27:16.416084Z","shell.execute_reply":"2022-05-24T21:27:16.444352Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 15), sharex = True)\nsns.barplot(data= df_IR_count, x = 'date', y = 'Tweet_count_ireland' , ax = ax1, palette = \"crest\")\nsns.barplot(data= df_UK_count, x = 'date', y = 'Tweet_count_UK', ax = ax2,palette=\"light:b\")\nticks= plt.setp(ax2.get_xticklabels(), rotation=45)\nax1.set_title('Tweet Frequency in Ireland and UK during March 2020')\n#title = plt.title('Tweet Frequency in Ireland and UK during March 2020')\nplt.savefig('#4.1_Tweet_frequency_comparision.png',bbox = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:27:16.452820Z","iopub.execute_input":"2022-05-24T21:27:16.453401Z","iopub.status.idle":"2022-05-24T21:27:18.832321Z","shell.execute_reply.started":"2022-05-24T21:27:16.453360Z","shell.execute_reply":"2022-05-24T21:27:18.831330Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Task 4.2: Characterise 3 days\n\n* Ireland: March 17 Tuesday\n* Ireland: March 12 Thursday --X March 27 Friday\n* UK : March 20 Friday\n","metadata":{}},{"cell_type":"code","source":"df_IR_17 = df.loc[(df['country'] == \"Ireland\") & (df['created_at'].str[4:10] == 'Mar 17'), 'processed_text']\ndf_IR_17","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:29:21.827798Z","iopub.execute_input":"2022-05-24T21:29:21.828708Z","iopub.status.idle":"2022-05-24T21:29:29.384212Z","shell.execute_reply.started":"2022-05-24T21:29:21.828624Z","shell.execute_reply":"2022-05-24T21:29:29.383696Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_IR_27 = df.loc[(df['country'] == \"Ireland\") & (df['created_at'].str[4:10] == 'Mar 27'), 'processed_text']\ndf_UK_20 = df.loc[(df['country'] == \"United Kingdom\") & (df['created_at'].str[4:10] == 'Mar 20'), 'processed_text']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:29:29.385352Z","iopub.execute_input":"2022-05-24T21:29:29.385810Z","iopub.status.idle":"2022-05-24T21:29:42.702639Z","shell.execute_reply.started":"2022-05-24T21:29:29.385783Z","shell.execute_reply":"2022-05-24T21:29:42.701600Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_IR_12 = df.loc[(df['country'] == \"Ireland\") & (df['created_at'].str[4:10] == 'Mar 12'), 'processed_text']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:29:42.704626Z","iopub.execute_input":"2022-05-24T21:29:42.705620Z","iopub.status.idle":"2022-05-24T21:29:49.988138Z","shell.execute_reply.started":"2022-05-24T21:29:42.705571Z","shell.execute_reply":"2022-05-24T21:29:49.986657Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_UK_17 = df.loc[(df['country'] == \"Ireland\") & (df['created_at'].str[4:10] == 'Mar 17'), 'processed_text']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:29:52.572404Z","iopub.execute_input":"2022-05-24T21:29:52.572723Z","iopub.status.idle":"2022-05-24T21:29:58.106058Z","shell.execute_reply.started":"2022-05-24T21:29:52.572686Z","shell.execute_reply":"2022-05-24T21:29:58.104533Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_UK_26 = df.loc[(df['country'] == \"United Kingdom\") & (df['created_at'].str[4:10] == 'Mar 26'), 'processed_text']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:29:58.107946Z","iopub.execute_input":"2022-05-24T21:29:58.108691Z","iopub.status.idle":"2022-05-24T21:30:04.246507Z","shell.execute_reply.started":"2022-05-24T21:29:58.108658Z","shell.execute_reply":"2022-05-24T21:30:04.245612Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n\n# making a huge string with processed data value\ntext = \" \".join(tweet for tweet in df_UK_26.values)\nstopwords = set(STOPWORDS)\nstopwords.update([\"s\", \"t\", \"will\", \"thank\",\"day\",\"m\",\"one\",\"good\",\"now\", \"thanks\", \"time\", \"amp\", \"re\", \"u\", \"people\"\n                 ,\"ireland\", \"new\", \"got\", \"know\", \"going\", \"go\", \"much\"])\n\nf, ax = plt.subplots(figsize= [20,10])\nplt.imshow(WordCloud(stopwords = stopwords,  collocations = False, width=2000,height=1000, \n                     background_color = 'white').generate(text), interpolation='bilinear', )\nplt.axis('off')\n#plt.savefig('26maruk.png', bbox = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:40:18.109704Z","iopub.execute_input":"2022-05-24T21:40:18.110223Z","iopub.status.idle":"2022-05-24T21:40:26.456843Z","shell.execute_reply.started":"2022-05-24T21:40:18.110178Z","shell.execute_reply":"2022-05-24T21:40:26.455741Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#plt.savefig('#4.2_Ireland_12_Mar.png', bbox = 'tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = set(STOPWORDS)\nstopwords.update([\"s\", \"t\", \"will\", \"thank\", \"well\",\"day\",\"m\",\"one\",\"good\",\"now\", \"thanks\", \"time\", \"amp\", \"re\", \"u\", \"people\"\n                 ,\"ireland\"])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:40:42.603418Z","iopub.execute_input":"2022-05-24T21:40:42.603719Z","iopub.status.idle":"2022-05-24T21:40:42.610224Z","shell.execute_reply.started":"2022-05-24T21:40:42.603688Z","shell.execute_reply":"2022-05-24T21:40:42.608722Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(tweet for tweet in df_IR_12.values)\nf, ax = plt.subplots(figsize= [20,10])\nplt.imshow(WordCloud(stopwords = stopwords,  collocations = False, width=2000,height=1000, background_color = 'white').generate(text), interpolation='bilinear', )\nplt.axis('off')\n#plt.savefig('#4.2_Ireland_12_Mar.png', bbox = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:40:45.750111Z","iopub.execute_input":"2022-05-24T21:40:45.752111Z","iopub.status.idle":"2022-05-24T21:40:50.118403Z","shell.execute_reply.started":"2022-05-24T21:40:45.752031Z","shell.execute_reply":"2022-05-24T21:40:50.117745Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nUK_mask = np.array(Image.open(\"../input/q4-images/UK_outline_edited.png\"))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:41:09.295812Z","iopub.execute_input":"2022-05-24T21:41:09.296411Z","iopub.status.idle":"2022-05-24T21:41:09.327068Z","shell.execute_reply.started":"2022-05-24T21:41:09.296358Z","shell.execute_reply":"2022-05-24T21:41:09.325514Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def transform_format(val):\n    if val == 0:\n        return 255\n    else:\n        return val","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:41:49.536718Z","iopub.execute_input":"2022-05-24T21:41:49.538080Z","iopub.status.idle":"2022-05-24T21:41:49.544363Z","shell.execute_reply.started":"2022-05-24T21:41:49.538007Z","shell.execute_reply":"2022-05-24T21:41:49.542596Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Transform your mask into a new one that will work with the function:\ntransformed_UK_mask = np.ndarray((UK_mask.shape[0],UK_mask.shape[1]), np.int32)\nprint(transformed_UK_mask)\nfor i in range(len(UK_mask)):\n    for j in range (len(transformed_UK_mask[i])):\n        transformed_UK_mask[i][j] = list(map(transform_format, UK_mask[i]))\nprint(transformed_UK_mask)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:41:51.976326Z","iopub.execute_input":"2022-05-24T21:41:51.976686Z","iopub.status.idle":"2022-05-24T21:41:52.021655Z","shell.execute_reply.started":"2022-05-24T21:41:51.976649Z","shell.execute_reply":"2022-05-24T21:41:52.019452Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"stopwords = set(STOPWORDS)\nstopwords.update([\"s\", \"t\", \"will\", \"thank\",\"day\",\"m\",\"one\",\"good\",\"now\", \"thanks\", \"time\", \"amp\", \"re\", \"u\", \"people\"\n                 ,\"ireland\", \"need\", \"going\", \"work\", \"think\", \"know\", \"today\", \"love\", \"great\", \"hope\", \"ve\", \"go\", \n                  \"see\"])\n\ntext2 = \" \".join(tweet for tweet in df_UK_26.values)\nf, ax = plt.subplots(figsize= [20,10])\nplt.imshow(WordCloud(stopwords = stopwords,  collocations = True, width=2000,height=1000,\n                     background_color = 'white', mask=UK_mask, contour_width=3, contour_color = 'firebrick').generate(text2), interpolation='bilinear' )\nplt.axis('off')\nplt.savefig('#4.2_UK_26_Mar.png', bbox = 'tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T21:42:18.120925Z","iopub.execute_input":"2022-05-24T21:42:18.122162Z","iopub.status.idle":"2022-05-24T21:42:39.636481Z","shell.execute_reply.started":"2022-05-24T21:42:18.122054Z","shell.execute_reply":"2022-05-24T21:42:39.635787Z"},"trusted":true},"execution_count":34,"outputs":[]}]}